{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_amazon2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielDLX/DLfinal/blob/master/bert_amazon2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx1UegbARdfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "c5427a81-24f1-4989-e9d5-80ec60790435"
      },
      "source": [
        "# 导入包\n",
        "# transformer提供了一些训练好的模型，可以很方便的使用。\n",
        "!pip install transformers\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import tqdm\n",
        "# 使用分类的模型，增加了一个head用于分类。\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.0rc4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0giWORbAR2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "outputId": "72403e8f-0e74-45fb-fc15-2c95cc62b81f"
      },
      "source": [
        "# 导入现成的模型和分词器\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2) # 分类类别数\n",
        "model.summary()\n",
        "model.config"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier', 'dropout_37']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  109482240 \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "=================================================================\n",
            "Total params: 109,483,778\n",
            "Trainable params: 109,483,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"architectures\": [\n",
              "    \"BertForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JasImGjMRqFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 数据链接，可以在 https://course.fast.ai/datasets 找到。\n",
        "# agnews数据集，类别，标题，描述。\n",
        "am2_url = 'https://s3.amazonaws.com/fast-ai-nlp/amazon_review_polarity_csv.tgz'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSHGPWGtSA7E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2eb8867b-fc5d-426e-9990-400995306aa2"
      },
      "source": [
        "# 下载数据，并指定此时数据集的目录\n",
        "am2_zip_file = tf.keras.utils.get_file(origin=am2_url,fname='amazon_review_polarity_csv.tgz', extract=True)\n",
        "base_dir = os.path.join(os.path.dirname(am2_zip_file), 'amazon_review_polarity_csv')\n",
        "os.listdir(base_dir)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['readme.txt', 'test.csv', 'train.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ0hxlyr2sPe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "cde0682b-765d-434a-927d-3e40f3d25593"
      },
      "source": [
        "# 浏览一下readme\n",
        "f = open(os.path.join(base_dir, 'readme.txt'))\n",
        "con = f.readlines()\n",
        "print(con)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Amazon Review Polaridy Dataset\\n', '\\n', 'Version 3, Updated 09/09/2015\\n', '\\n', 'ORIGIN\\n', '\\n', 'The Amazon reviews dataset consists of reviews from amazon. The data span a period of 18 years, including ~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review. For more information, please refer to the following paper: J. McAuley and J. Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. RecSys, 2013.\\n', '\\n', 'The Amazon reviews polarity dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the above dataset. It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\\n', '\\n', '\\n', 'DESCRIPTION\\n', '\\n', 'The Amazon reviews polarity dataset is constructed by taking review score 1 and 2 as negative, and 4 and 5 as positive. Samples of score 3 is ignored. In the dataset, class 1 is the negative and class 2 is the positive. Each class has 1,800,000 training samples and 200,000 testing samples.\\n', '\\n', 'The files train.csv and test.csv contain all the training samples as comma-sparated values. There are 3 columns in them, corresponding to class index (1 or 2), review title and review text. The review title and text are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). New lines are escaped by a backslash followed with an \"n\" character, that is \"\\\\n\".\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BYSiEligwzF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "0a070635-5998-4139-8061-c3b09c04008d"
      },
      "source": [
        "# 读取数据\n",
        "# base_dir = ''\n",
        "train = pd.read_csv(os.path.join(base_dir, 'train.csv'), header=None)\n",
        "print(len(train))\n",
        "print(train.head())\n",
        "test = pd.read_csv(os.path.join(base_dir, 'test.csv'), header=None)\n",
        "print(len(test))\n",
        "print(test.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3600000\n",
            "   0  ...                                                  2\n",
            "0  2  ...  This sound track was beautiful! It paints the ...\n",
            "1  2  ...  I'm reading a lot of reviews saying that this ...\n",
            "2  2  ...  This soundtrack is my favorite music of all ti...\n",
            "3  2  ...  I truly like this soundtrack and I enjoy video...\n",
            "4  2  ...  If you've played the game, you know how divine...\n",
            "\n",
            "[5 rows x 3 columns]\n",
            "400000\n",
            "   0  ...                                                  2\n",
            "0  2  ...  My lovely Pat has one of the GREAT voices of h...\n",
            "1  2  ...  Despite the fact that I have only played a sma...\n",
            "2  1  ...  I bought this charger in Jul 2003 and it worke...\n",
            "3  2  ...  Check out Maha Energy's website. Their Powerex...\n",
            "4  2  ...  Reviewed quite a bit of the combo players and ...\n",
            "\n",
            "[5 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh_bZhMsuZB-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "0c1f2200-0b4d-468d-e042-50fb67076ff8"
      },
      "source": [
        "# transformers自带的tokenizer中的encoder会把一段文本进行编码，然后增加上CLS和SEP，其中CLS的id是101，SEP的编码是102,PAD是0。\n",
        "# 所以  a   dog   is  not   a   table\n",
        "# [cls]  a   dog   is  not   a   table  [sep]\n",
        "# 101   1037  3899  2003 2025  1037  2795   102   0  0  0  ...  0 \n",
        "# %pprint #让列表横过来，好看一些。\n",
        "tokenizer.encode(text='a dog is not a table', padding='max_length',max_length=512)[:20]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 1037,\n",
              " 3899,\n",
              " 2003,\n",
              " 2025,\n",
              " 1037,\n",
              " 2795,\n",
              " 102,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBGAtYI3SR8G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "24026b19-857b-45da-97e8-32afcde4cb55"
      },
      "source": [
        "# 预处理数据\n",
        "# 按照上面的例子把数据集中的文本进行分词处理，并且得到对应的labels。\n",
        "max_length = 512\n",
        "# 数据中有的文本长度大于512，如果把max_length进一步放大内存就不够了，所以选择截断\n",
        "max_length_temp = max_length - 2\n",
        "for i in range(train.shape[0]):\n",
        "  if len(train[2][i]) > max_length_temp:\n",
        "    train[2][i] = train[2][i][0 : max_length_temp]\n",
        "for i in range(test.shape[0]):\n",
        "  if len(test[2][i]) > max_length_temp:\n",
        "    test[2][i] = test[2][i][0 : max_length_temp]\n",
        "train_ids = [tokenizer.encode(text=sent, padding='max_length', max_length=max_length, return_tensors=\"tf\") for sent in tqdm.notebook.tqdm(train[2])]\n",
        "test_ids = [tokenizer.encode(text=sent, padding='max_length', max_length=max_length, return_tensors=\"tf\") for sent in tqdm.notebook.tqdm(test[2])]\n",
        "train_labels = train[0].values - 1\n",
        "test_labels = test[0].values - 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SucNjO6sQT9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 把数据转成tensorflow张量\n",
        "# train_ids是tf.Tensor组成得列表，所以用concat组合一下就行\n",
        "train_ids = tf.concat(train_ids, 0)\n",
        "# 把train_mask初始化为1，然后把train_ids等于0（PAD的部分）对应的值赋为0\n",
        "train_mask = tf.ones(train_ids.shape)\n",
        "train_mask = tf.where(tf.math.greater(train_ids, 0), train_mask, 0)\n",
        "# labels本身是numpy数组，转为tf.Tensor\n",
        "train_labels = tf.convert_to_tensor(train_labels)\n",
        "\n",
        "# 测试集的处理同理test\n",
        "test_ids = tf.concat(test_ids, 0)\n",
        "test_mask = tf.ones(test_ids.shape)\n",
        "test_mask = tf.where(tf.math.greater(test_ids, 0), test_mask, 0)\n",
        "test_labels = tf.convert_to_tensor(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0aaG0cYFj2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_ids[0])\n",
        "print(train_mask[0])\n",
        "print(train_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XUKs5A_19zE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 训练参数\n",
        "epochs = 1\n",
        "batch_size = 4\n",
        "validation_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F2dKeUi3nGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 模型编译\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC9Y5UGV8VT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 模型训练\n",
        "model.fit(x=[train_ids, train_mask], \n",
        "     y=train_labels, \n",
        "     batch_size=batch_size, \n",
        "     epochs=epochs, \n",
        "     verbose=1, \n",
        "     callbacks=None,\n",
        "     validation_split=validation_rate, \n",
        "     validation_data=None, \n",
        "     shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly4iDgmklcy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 模型测试\n",
        "model.evaluate(x=[test_ids, test_mask],\n",
        "        y=test_labels, \n",
        "        batch_size=4, \n",
        "        verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}