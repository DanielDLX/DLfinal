{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_yelpFull.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buzhangjiuzhou/DLfinal/blob/master/bert_yelpFull_retrained_another.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx1UegbARdfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "b04c5070-7777-4f7a-d049-b9f2f0319253"
      },
      "source": [
        "# 导入包\n",
        "# transformer提供了一些训练好的模型，可以很方便的使用。\n",
        "!pip install transformers\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import tqdm\n",
        "import random \n",
        "import math\n",
        "# 使用分类的模型，增加了一个head用于分类。\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0giWORbAR2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        },
        "outputId": "b73e03a2-d982-424e-e133-ccda6e8caabe"
      },
      "source": [
        "# 导入现成的模型和分词器\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\n",
        "model.summary()\n",
        "model.config"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertForSequenceClassification: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier', 'dropout_75']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  109482240 \n",
            "_________________________________________________________________\n",
            "dropout_75 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  3845      \n",
            "=================================================================\n",
            "Total params: 109,486,085\n",
            "Trainable params: 109,486,085\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"architectures\": [\n",
              "    \"BertForMaskedLM\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\",\n",
              "    \"3\": \"LABEL_3\",\n",
              "    \"4\": \"LABEL_4\"\n",
              "  },\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2,\n",
              "    \"LABEL_3\": 3,\n",
              "    \"LABEL_4\": 4\n",
              "  },\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"vocab_size\": 30522\n",
              "}\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JasImGjMRqFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 数据链接，可以在 https://course.fast.ai/datasets 找到。\n",
        "# agnews数据集，类别，标题，描述。\n",
        "ag_url = 'https://s3.amazonaws.com/fast-ai-nlp/yelp_review_full_csv.tgz'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSHGPWGtSA7E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f4c16fcc-606c-452a-bece-c79975abda6a"
      },
      "source": [
        "# 下载数据，并指定此时数据集的目录\n",
        "ag_zip_file = tf.keras.utils.get_file(origin=ag_url,fname='yelp_review_full_csv.tgz', extract=True)\n",
        "base_dir = os.path.join(os.path.dirname(ag_zip_file), 'yelp_review_full_csv')\n",
        "os.listdir(base_dir)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['readme.txt', 'test.csv', 'train.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BYSiEligwzF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "2886d1cb-c66a-4f89-f9f3-4679cc57834b"
      },
      "source": [
        "# 读取数据\n",
        "# base_dir = ''\n",
        "train = pd.read_csv(os.path.join(base_dir, 'train.csv'), header=None)\n",
        "print(train)\n",
        "print(len(train))\n",
        "print(train.head())\n",
        "# train = train.sample(n=50000)\n",
        "print(len(train))\n",
        "test = pd.read_csv(os.path.join(base_dir, 'test.csv'), header=None)\n",
        "print(len(test))\n",
        "print(test.head())\n",
        "# f = open(os.path.join(base_dir, 'classes.txt'))\n",
        "# classes = f.readlines()\n",
        "# classes = [s.strip() for s in classes]\n",
        "# print(classes)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        0                                                  1\n",
            "0       5  dr. goldberg offers everything i look for in a...\n",
            "1       2  Unfortunately, the frustration of being Dr. Go...\n",
            "2       4  Been going to Dr. Goldberg for over 10 years. ...\n",
            "3       4  Got a letter in the mail last week that said D...\n",
            "4       1  I don't know what Dr. Goldberg was like before...\n",
            "...    ..                                                ...\n",
            "649995  5  I had a sprinkler that was gushing... pipe bro...\n",
            "649996  1  Phone calls always go to voicemail and message...\n",
            "649997  1  Looks like all of the good reviews have gone t...\n",
            "649998  5  I was able to once again rely on Yelp to provi...\n",
            "649999  1  I have been using this company for 11 months. ...\n",
            "\n",
            "[650000 rows x 2 columns]\n",
            "650000\n",
            "   0                                                  1\n",
            "0  5  dr. goldberg offers everything i look for in a...\n",
            "1  2  Unfortunately, the frustration of being Dr. Go...\n",
            "2  4  Been going to Dr. Goldberg for over 10 years. ...\n",
            "3  4  Got a letter in the mail last week that said D...\n",
            "4  1  I don't know what Dr. Goldberg was like before...\n",
            "650000\n",
            "50000\n",
            "   0                                                  1\n",
            "0  1  I got 'new' tires from them and within two wee...\n",
            "1  1  Don't waste your time.  We had two different p...\n",
            "2  1  All I can say is the worst! We were the only 2...\n",
            "3  1  I have been to this restaurant twice and was d...\n",
            "4  1  Food was NOT GOOD at all! My husband & I ate h...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh_bZhMsuZB-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "d0bddd0b-352c-41d1-ae9d-44917f39c871"
      },
      "source": [
        "# transformers自带的tokenizer中的encoder会把一段文本进行编码，然后增加上CLS和SEP，其中CLS的id是101，SEP的编码是102,PAD是0。\n",
        "# 所以  a   dog   is  not   a   table\n",
        "# [cls]  a   dog   is  not   a   table  [sep]\n",
        "# 101   1037  3899  2003 2025  1037  2795   102\n",
        "%pprint #让列表横过来，好看一些。\n",
        "tokenizer.encode(text='a dog is not a table', padding='max_length',max_length=512)[:20]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pretty printing has been turned ON\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 1037,\n",
              " 3899,\n",
              " 2003,\n",
              " 2025,\n",
              " 1037,\n",
              " 2795,\n",
              " 102,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBGAtYI3SR8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 预处理数据\n",
        "# 按照上面的例子把数据集中的文本进行处理，并且得到对应的labels。\n",
        "max_length = 256\n",
        "# 把太长的句子给截断，防止后面出问题。\n",
        "max_length_temp = max_length - 2\n",
        "train_ids = []\n",
        "test_ids = []\n",
        "for i in range(train.shape[0]):\n",
        "  if len(train[1][i]) > max_length_temp:\n",
        "    train_ids.append(tokenizer.encode(text=train[1][i][0 : max_length_temp], padding='max_length', max_length=max_length, return_tensors=\"tf\"))\n",
        "  else:\n",
        "    train_ids.append(tokenizer.encode(text=train[1][i], padding='max_length', max_length=max_length, return_tensors=\"tf\"))\n",
        "for i in range(test.shape[0]):\n",
        "  if len(test[1][i]) > max_length_temp:\n",
        "    test_ids.append(tokenizer.encode(text=test[1][i][0 : max_length_temp], padding='max_length', max_length=max_length, return_tensors=\"tf\"))\n",
        "  else:\n",
        "    test_ids.append(tokenizer.encode(text=test[1][i], padding='max_length', max_length=max_length, return_tensors=\"tf\"))\n",
        "train_labels = train[0].values - 1\n",
        "test_labels = test[0].values - 1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwPvs5rlK-NC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "outputId": "793c020e-dc17-4ec3-e0df-7af05ed8b394"
      },
      "source": [
        "# 把数据转成tensorflow张量\n",
        "# train_ids是tf.Tensor组成得列表，所以用concat组合一下就行\n",
        "train_ids = tf.concat(train_ids, 0)\n",
        "# 把train_mask初始化为1，然后把train_ids等于0（PAD的部分）对应的值赋为0\n",
        "train_mask = tf.ones(train_ids.shape)\n",
        "train_mask = tf.where(tf.math.greater(train_ids, 0), train_mask, 0)\n",
        "# labels本身是numpy数组，转为tf.Tensor\n",
        "train_labels = tf.convert_to_tensor(train_labels)\n",
        "\n",
        "# 测试集的处理同理test\n",
        "test_ids = tf.concat(test_ids, 0)\n",
        "test_mask = tf.ones(test_ids.shape)\n",
        "test_mask = tf.where(tf.math.greater(test_ids, 0), test_mask, 0)\n",
        "test_labels = tf.convert_to_tensor(test_labels)\n",
        "\n",
        "print(train_ids[0])\n",
        "print(train_mask[0])\n",
        "print(train_labels[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[  101  2852  1012 18522  4107  2673  1045  2298  2005  1999  1037  2236\n",
            " 18742  1012  2002  1005  1055  3835  1998  3733  2000  2831  2000  2302\n",
            "  2108  9161  6026  1025  2002  1005  1055  2467  2006  2051  1999  3773\n",
            "  2010  5022  1025  2002  1005  1055  6989  2007  1037  2327  1011 18624\n",
            "  2902  1006 27935  1007  2029  2026  3008  2031  4541  2000   102     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0], shape=(256,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(256,), dtype=float32)\n",
            "tf.Tensor(4, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XUKs5A_19zE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 训练参数\n",
        "epochs = 1\n",
        "batch_size = 16\n",
        "validation_rate = 0.1"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F2dKeUi3nGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 训练\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO7-iNZtl0Ib",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "ba7a1541-8978-417e-dc96-b69f8e60d578"
      },
      "source": [
        "# 模型训练\n",
        "model.fit(x=[train_ids, train_mask], \n",
        "     y=train_labels, \n",
        "     batch_size=batch_size, \n",
        "     epochs=epochs, \n",
        "     verbose=1, \n",
        "    #  callbacks=None,\n",
        "    #  validation_split=validation_rate, \n",
        "    #  validation_data=None, \n",
        "     shuffle=True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40625/40625 [==============================] - 19896s 490ms/step - loss: 0.9621 - accuracy: 0.5791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f391ff9ea20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLiDetx81-ir",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b31c410e-03ec-4ab0-def0-cc598ed47026"
      },
      "source": [
        "# 模型测试\n",
        "model.evaluate(x=[test_ids, test_mask],\n",
        "        y=test_labels, \n",
        "        batch_size=4, \n",
        "        verbose=1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12500/12500 [==============================] - 547s 44ms/step - loss: 0.9252 - accuracy: 0.5950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9252158403396606, 0.594980001449585]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDB1B7eULt9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}