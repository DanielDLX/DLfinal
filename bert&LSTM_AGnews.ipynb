{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert&LSTM_AGnews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielDLX/DLfinal/blob/master/bert%26LSTM_AGnews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx1UegbARdfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "286104dc-eed2-4c16-ca60-06b1a597b6b5"
      },
      "source": [
        "# 导入包\n",
        "# transformer提供了一些训练好的模型，可以很方便的使用。\n",
        "!pip install transformers\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import tqdm\n",
        "# 使用分类的模型，增加了一个head用于分类。\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "% matplotlib inline\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJRP6NYeCLxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分词器\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JasImGjMRqFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 数据链接，可以在 https://course.fast.ai/datasets 找到。\n",
        "# agnews数据集，类别，标题，描述。\n",
        "ag_url = 'https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSHGPWGtSA7E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f75bf679-0607-4fdc-f6d7-b3da3e74c583"
      },
      "source": [
        "# 下载数据，并指定此时数据集的目录\n",
        "ag_zip_file = tf.keras.utils.get_file(origin=ag_url,fname='ag_news_csv.tgz', extract=True)\n",
        "base_dir = os.path.join(os.path.dirname(ag_zip_file), 'ag_news_csv')\n",
        "os.listdir(base_dir)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['readme.txt', 'classes.txt', 'test.csv', 'train.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BYSiEligwzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 读取数据\n",
        "# 为了方便测试代码的时候少用几个数据,1000个数据可以让准确率上升到0.68\n",
        "num = 3400\n",
        "train = pd.read_csv(os.path.join(base_dir, 'train.csv'), header=None)\n",
        "# train = train.sample(n=num)\n",
        "print(len(train))\n",
        "print(train.head())\n",
        "test = pd.read_csv(os.path.join(base_dir, 'test.csv'), header=None)\n",
        "# test = test.sample(n=num)\n",
        "print(len(test))\n",
        "print(test.head())\n",
        "f = open(os.path.join(base_dir, 'classes.txt'))\n",
        "classes = f.readlines()\n",
        "classes = [s.strip() for s in classes]\n",
        "print(classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBGAtYI3SR8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 预处理数据\n",
        "# 按照上面的例子把数据集中的文本进行分词处理，并且得到对应的labels。\n",
        "max_length = 256\n",
        "max_length_temp = max_length - 2\n",
        "train_ids = []\n",
        "test_ids = []\n",
        "for i in tqdm.notebook.trange(train.shape[0]):\n",
        "  if len(train[2][train[2].index[i]]) > max_length_temp:\n",
        "    train_ids.append(tokenizer.encode(text=train[2][train[2].index[i]][0 : max_length_temp], padding='max_length', max_length=max_length, return_tensors=\"tf\"))\n",
        "  else:\n",
        "    train_ids.append(tokenizer.encode(text=train[2][train[2].index[i]], padding='max_length', max_length=max_length, return_tensors=\"tf\"))\n",
        "for i in tqdm.notebook.trange(test.shape[0]):\n",
        "  if len(test[2][test[2].index[i]]) > max_length_temp:\n",
        "    test_ids.append(tokenizer.encode(text=test[2][test[2].index[i]][0 : max_length_temp], padding='max_length', max_length=max_length, return_tensors=\"tf\"))\n",
        "  else:\n",
        "    test_ids.append(tokenizer.encode(text=test[2][test[2].index[i]], padding='max_length', max_length=max_length, return_tensors=\"tf\"))\n",
        "train_labels = train[0].values - 1\n",
        "test_labels = test[0].values - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SucNjO6sQT9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 把数据转成tensorflow张量\n",
        "# train_ids是tf.Tensor组成得列表，所以用concat组合一下就行\n",
        "train_ids = tf.concat(train_ids, 0)\n",
        "# 把train_mask初始化为1，然后把train_ids等于0（PAD的部分）对应的值赋为0\n",
        "train_mask = tf.ones(train_ids.shape)\n",
        "train_mask = tf.where(tf.math.greater(train_ids, 0), train_mask, 0)\n",
        "# labels本身是numpy数组，转为tf.Tensor\n",
        "train_labels = tf.convert_to_tensor(train_labels)\n",
        "\n",
        "# 测试集的处理同理test\n",
        "test_ids = tf.concat(test_ids, 0)\n",
        "test_mask = tf.ones(test_ids.shape)\n",
        "test_mask = tf.where(tf.math.greater(test_ids, 0), test_mask, 0)\n",
        "test_labels = tf.convert_to_tensor(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0aaG0cYFj2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_ids[0])\n",
        "print(train_mask[0])\n",
        "print(train_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XUKs5A_19zE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 训练参数\n",
        "epochs = 2\n",
        "batch_size = 16\n",
        "validation_rate = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMGgvWXm3hsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 模型放在这里\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4, output_hidden_states=True) # 分类类别数\n",
        "model.summary()\n",
        "model.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx-oJlYK3nXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 简单分类算法\n",
        "model_compare = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(30522, 768),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(4)\n",
        "])\n",
        "model_compare.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F2dKeUi3nGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 模型编译\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK0ghwElncWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn_rate(ep):\n",
        "  return 0.001 / (math.pow(2, ep))\n",
        "LRS = tf.keras.callbacks.LearningRateScheduler(learn_rate)\n",
        "optimizer_compare = tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=1e-08, clipnorm=1.0)\n",
        "model_compare.compile(optimizer=optimizer_compare, loss=loss, metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJecHZU6ap6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# history是按照一个epoch记录一次，间隔太大了，所以手动重写一个基于batch的\n",
        "class BatchCallback(tf.keras.callbacks.History):\n",
        "  def __init__(self):\n",
        "    self.batch = []\n",
        "    self.history = {'loss':[], 'accuracy':[]}\n",
        "  def on_train_batch_end(self, batch, logs=None):\n",
        "    self.batch.append(batch)\n",
        "    for k, v in logs.items():\n",
        "      self.history[k].append(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC9Y5UGV8VT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 模型训练\n",
        "history = model.fit(x=[train_ids, train_mask], \n",
        "     y=train_labels, \n",
        "     batch_size=batch_size, \n",
        "     epochs=epochs, \n",
        "     verbose=1, \n",
        "     callbacks=[BatchCallback()],\n",
        "     shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILJtTOV3UkM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_compare = model_compare.fit(\n",
        "    x=[train_ids, train_mask],\n",
        "    y=train_labels,\n",
        "    batch_size=batch_size, \n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    callbacks=[BatchCallback(), LRS],\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly4iDgmklcy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 模型测试\n",
        "model.evaluate(x=[test_ids, test_mask],\n",
        "        y=test_labels, \n",
        "        batch_size=batch_size, \n",
        "        verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8ME3Dv0U-UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_compare.evaluate(x=[test_ids, test_mask],\n",
        "        y=test_labels, \n",
        "        batch_size=batch_size, \n",
        "        verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqvrDdKplstd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 画图\n",
        "def plot_graphs(history,history_compare, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history_compare.history[metric])\n",
        "  plt.xlabel(\"Batches\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.show()\n",
        "plot_graphs(history, history_compare, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi8sc_xmsqs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index = None\n",
        "for idx, layer in enumerate(model_compare.layers):\n",
        "  print(idx, layer.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5bV2M4U5Ati",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 得到RNN模型embedding的方法，get embedding层后，输入词的id，得到embeding\n",
        "# 用index更好\n",
        "embedding_compare = model_compare.get_layer(index=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwViryLLo3h3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 得到每个类的embedding\n",
        "avg_pool1 = tf.keras.layers.AveragePooling2D(pool_size=(max_length, 1))\n",
        "N = tf.shape(train_ids)[0] // 1000\n",
        "embs_list = []\n",
        "for i in range(N):\n",
        "  embs = embedding_compare(train_ids[i*1000:(i+1)*1000])\n",
        "  embs = tf.reshape(avg_pool1(tf.reshape(embs,[-1,256,768,1])), [-1,768])\n",
        "  embs_list.append(embs)\n",
        "embs = embedding_compare(train_ids[N*1000:])\n",
        "embs = tf.reshape(avg_pool1(tf.reshape(embs,[-1,256,768,1])), [-1,768])\n",
        "embs_list.append(embs)\n",
        "seq_embs_compare = tf.concat(embs_list, 0)\n",
        "class_embedding_compare = np.zeros([4,768])\n",
        "for i in range(4):\n",
        "  index = train_labels == i\n",
        "  class_embs = seq_embs_compare[index]\n",
        "  avg_pool2 = tf.keras.layers.AveragePooling2D(pool_size=(tf.shape(class_embs)[0], 1))\n",
        "  class_embs = tf.reshape(class_embs,[1,-1,768,1])\n",
        "  class_embs = avg_pool2(class_embs)\n",
        "  class_embedding_compare[i] = tf.reshape(class_embs, [768]).numpy()\n",
        "print(class_embedding_compare)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsjVqfIDuRFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 获得bert Embedding的方式，这里得到是整个embedding张量\n",
        "embeddings = model.get_layer('bert').embeddings.word_embeddings\n",
        "\n",
        "# 定义函数：通过词id 得到embedding\n",
        "def get_embedding(input_ids, embeddings=embeddings):\n",
        "  embeds = tf.gather(embeddings, input_ids)\n",
        "  return embeds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "calkKhCKo7f7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 得到每个类的embedding\n",
        "N = tf.shape(train_ids)[0] // 1000\n",
        "embs_list = []\n",
        "for i in range(N):\n",
        "  embs = get_embedding(train_ids[i*1000:(i+1)*1000])\n",
        "  embs = tf.reshape(avg_pool1(tf.reshape(embs,[-1,256,768,1])), [-1,768])\n",
        "  embs_list.append(embs)\n",
        "embs = get_embedding(train_ids[N*1000:])\n",
        "embs = tf.reshape(avg_pool1(tf.reshape(embs,[-1,256,768,1])), [-1,768])\n",
        "embs_list.append(embs)\n",
        "seq_embs = tf.concat(embs_list, 0)\n",
        "print(seq_embs)\n",
        "class_embedding = np.zeros([4,768])\n",
        "for i in range(4):\n",
        "  index = train_labels == i\n",
        "  class_embs = seq_embs[index]\n",
        "  avg_pool2 = tf.keras.layers.AveragePooling2D(pool_size=(tf.shape(class_embs)[0], 1))\n",
        "  class_embs = tf.reshape(class_embs,[1,-1,768,1])\n",
        "  class_embs = avg_pool2(class_embs)\n",
        "  class_embedding[i] = tf.reshape(class_embs, [768]).numpy()\n",
        "print(class_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNMDV_VX7Sso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 类表征的余弦相似性\n",
        "res = sklearn.metrics.pairwise.cosine_similarity(class_embedding, class_embedding_compare)\n",
        "plt.imshow(res)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvufPWPnmT8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# t-SNE分析\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}